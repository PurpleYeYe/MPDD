import torch
import os
import json
from collections import OrderedDict
import torch.nn.functional as F
from models.base_model import BaseModel
from models.networks.lstm import LSTMEncoder
from models.networks.classifier import FcClassifier
from models.networks.audio_attention import AudioAttNet
from models.networks.conv_attn_encoder import ConvAttnEncoder
from models.utils.config import OptConfig
import math
import torch.nn as nn


class ourModel(BaseModel, nn.Module):
    """
    è¿™æ®µä»£ç æ˜¯ä½ é¡¹ç›®çš„æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ç±» ourModel çš„æ„é€ å‡½æ•° __init__()ï¼Œå®šä¹‰äº†æ¨¡å‹çš„å®Œæ•´ç»“æ„å’Œè®­ç»ƒé…ç½®ã€‚
    å®ƒæ•´åˆäº†éŸ³é¢‘ã€è§†è§‰ã€ä¸ªæ€§åŒ–ç‰¹å¾ã€Transformer èåˆæ¨¡å—å’Œå¤šä¸ªåˆ†ç±»å™¨ï¼Œå¹¶é…ç½®äº†æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨ã€‚
    BaseModelï¼šä½ é¡¹ç›®ä¸­å®šä¹‰çš„åŸºç¡€æ¨¡å‹ç±»ï¼ˆå°è£…äº†è®­ç»ƒæ¡†æ¶ï¼‰ï¼›
    nn.Moduleï¼šPyTorch çš„åŸºç¡€æ¨¡å‹ç±»ã€‚
    """
    def __init__(self, opt):
        """Initialize the LSTM autoencoder class
        Parameters:
            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions
        """
        nn.Module.__init__(self)# æ˜¾å¼è°ƒç”¨ nn.Module çš„æ„é€ å‡½æ•°ï¼Œæ³¨å†Œæ¨¡å—ï¼›
        super().__init__(opt)# è°ƒç”¨ BaseModel.__init__(opt)ï¼Œä¼ å…¥é…ç½®ï¼Œåˆå§‹åŒ–æ¨¡å‹åŸºç±»é€»è¾‘ï¼ˆå¦‚ self.device, self.isTrain ç­‰ï¼‰ã€‚

        
        self.loss_names = []# loss_namesï¼šæŸå¤±å‡½æ•°çš„åå­—ï¼›
        self.model_names = []# model_namesï¼šæ¨¡å‹ç»„ä»¶çš„åå­—ï¼ˆç”¨äºä¿å­˜/åŠ è½½æ¨¡å‹ï¼‰ã€‚

        # acoustic model
        """
        åˆ›å»ºéŸ³é¢‘ç¼–ç å™¨
        LSTMEncoder æ˜¯éŸ³é¢‘ç‰¹å¾çš„æ—¶åºç¼–ç å™¨ï¼ˆä½¿ç”¨ LSTMï¼‰ï¼›
        è¾“å…¥ç»´åº¦ input_dim_a æ¥è‡ª .npy æ–‡ä»¶ shapeï¼›
        è¾“å‡ºç»´åº¦ embd_size_aï¼›
        ä½¿ç”¨æ–¹æ³•å¦‚ mean, last, attn ç­‰èšåˆç­–ç•¥ã€‚
        """
        self.netEmoA = LSTMEncoder(opt.input_dim_a, opt.embd_size_a, embd_method=opt.embd_method_a)
        self.model_names.append('EmoA')

        # visual model
        """
        åˆ›å»ºè§†è§‰ç¼–ç å™¨
        åŒç†ï¼Œç”¨äºç¼–ç è§†é¢‘ç‰¹å¾ï¼›
        è¾“å…¥ç»´åº¦ input_dim_vï¼Œè¾“å‡ºç»´åº¦ embd_size_vï¼Œä½¿ç”¨ embd_method_v èšåˆæ–¹å¼ã€‚
        """
        self.netEmoV = LSTMEncoder(opt.input_dim_v, opt.embd_size_v, opt.embd_method_v)
        self.model_names.append('EmoV')
        self.audio_attention = AudioAttNet(
            dim_aud=opt.input_dim_a,  # ä¾‹å¦‚ 64
            seq_len=5  # å¯¹åº”ä½ å¯¹é½çš„éŸ³é¢‘å¸§çª—å£
        ).to(self.device)
        # Transformer Fusion model
        """
        åˆ›å»º Transformer èåˆæ¨¡å—
        ç”¨äºèåˆéŸ³é¢‘å’Œè§†é¢‘çš„æ—¶åºç‰¹å¾ï¼›
        å‚æ•°æ¥è‡ªé…ç½®æ–‡ä»¶ï¼š
        hidden_sizeï¼šæ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ç»´åº¦ï¼›
        Transformer_headï¼šå¤šå¤´æ³¨æ„åŠ›æ•°é‡ï¼›
        Transformer_layersï¼šTransformer å±‚æ•°ï¼›
        è¾“å‡ºç»´åº¦ä¾æ—§ä¸º batch_size Ã— seq_len Ã— hidden_sizeã€‚
        """
        emo_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=opt.hidden_size, nhead=int(opt.Transformer_head), batch_first=True)
        self.netEmoFusion = torch.nn.TransformerEncoder(emo_encoder_layer, num_layers=opt.Transformer_layers)
        self.model_names.append('EmoFusion')

        # Classifier
        """
        åˆ†ç±»å™¨ç½‘ç»œ
        åˆ†ç±»å™¨æ˜¯ä¸€ä¸ªå¤šå±‚å…¨è¿æ¥ç½‘ç»œï¼š
        å±‚ç»“æ„æ¥è‡ªå­—ç¬¦ä¸² "128,64" â†’ [128, 64]ï¼›
        è¾“å…¥ç‰¹å¾ç»´åº¦ = seq_len Ã— hidden_size + 1024ï¼ˆæ‹¼æ¥ä¸ªæ€§åŒ–å‘é‡ï¼‰ï¼›
        feature_max_len æ˜¯æ—¶é—´æ­¥æ•°ï¼›
        1024 æ˜¯ä¸ªæ€§åŒ–åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚
        """
        cls_layers = list(map(lambda x: int(x), opt.cls_layers.split(',')))
        # cls_input_size = 5*opt.hidden_size, same with max-len
        cls_input_size = opt.feature_max_len * opt.hidden_size + 1024  # with personalized feature

        """
        ä¸»åˆ†ç±»å™¨ï¼ˆEmoCï¼‰
        è¾“å‡ºç»´åº¦æ˜¯åˆ†ç±»çš„ç±»åˆ«æ•°ï¼›
        ä½¿ç”¨äº¤å‰ç†µè®¡ç®—ä¸»è¦æŸå¤± emo_CEã€‚
        """
        self.netEmoC = FcClassifier(cls_input_size, cls_layers, output_dim=opt.emo_output_dim, dropout=opt.dropout_rate)
        self.model_names.append('EmoC')
        self.loss_names.append('emo_CE')
        """
        è¾…åŠ©åˆ†ç±»å™¨ï¼ˆEmoCFï¼‰
        é€šå¸¸ä¸ Focal Loss æ­é…ä½¿ç”¨ï¼Œé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡ï¼›
        æ˜¯ä¸€ç§å¢å¼ºç­–ç•¥ï¼Œä¹Ÿå¯èƒ½ç”¨äº ICLï¼ˆInstance Contrastive Learningï¼‰ç»“æ„ã€‚
        """
        self.netEmoCF = FcClassifier(cls_input_size, cls_layers, output_dim=opt.emo_output_dim, dropout=opt.dropout_rate)
        self.model_names.append('EmoCF')
        self.loss_names.append('EmoF_CE')
        """
        ç½®æ¸©åº¦å‚æ•°
        é€šå¸¸ç”¨äº softmax æ¸©åº¦ç¼©æ”¾ï¼ˆä¾‹å¦‚åœ¨ contrastive learning æˆ–è’¸é¦ä¸­ï¼‰ï¼›
        åç»­æ¨¡å‹ä¸­å¯èƒ½ç”¨äºè°ƒèŠ‚ logitsã€‚
        """
        self.temperature = opt.temperature


        # self.device = 'cpu'
        # self.netEmoA = self.netEmoA.to(self.device)
        # self.netEmoV = self.netEmoV.to(self.device)
        # self.netEmoFusion = self.netEmoFusion.to(self.device)
        # self.netEmoC = self.netEmoC.to(self.device)
        # self.netEmoCF = self.netEmoCF.to(self.device)
        """
        è®¾ç½®æŸå¤±å‡½æ•°
        é»˜è®¤ä½¿ç”¨æ ‡å‡†äº¤å‰ç†µã€‚
        """
        self.criterion_ce = torch.nn.CrossEntropyLoss()
        """
        è®­ç»ƒæ¨¡å¼ä¸‹ï¼šåˆå§‹åŒ–ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰
        å¦‚æœå¯ç”¨äº† ICLï¼Œåˆ™ä½¿ç”¨è‡ªå®šä¹‰çš„ Focal_Loss()ï¼›
        å¦åˆ™ç›´æ¥ä½¿ç”¨ä¸¤æ¬¡æ ‡å‡†äº¤å‰ç†µï¼›
        ce_weight, focal_weight æ§åˆ¶æœ€ç»ˆæŸå¤±ä¸­çš„åŠ æƒæ¯”ä¾‹ã€‚
        """
        if self.isTrain:
            if not opt.use_ICL:
                self.criterion_ce = torch.nn.CrossEntropyLoss()
                self.criterion_focal = torch.nn.CrossEntropyLoss() 
            else:
                self.criterion_ce = torch.nn.CrossEntropyLoss()
                self.criterion_focal = Focal_Loss()
            """
            åˆ›å»ºä¼˜åŒ–å™¨
            æ”¶é›†æ‰€æœ‰å­æ¨¡å—çš„å‚æ•°ï¼›
            ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼›
            å‚æ•°ï¼šå­¦ä¹ ç‡ã€beta1 ä»é…ç½®æ–‡ä»¶ä¸­è·å–ã€‚
            """
            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.
            paremeters = [{'params': getattr(self, 'net' + net).parameters()} for net in self.model_names]
            self.optimizer = torch.optim.Adam(paremeters, lr=opt.lr, betas=(opt.beta1, 0.999))
            self.optimizers.append(self.optimizer)
            self.ce_weight = opt.ce_weight
            self.focal_weight = opt.focal_weight

        # modify save_dir
        """
        è®¾ç½®æ¨¡å‹ä¿å­˜è·¯å¾„
        è®¾ç½®æ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„ï¼›
        å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç›®å½•ã€‚
        """
        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name) 
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)  


    def post_process(self):
        """
        ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½ç¼–ç å™¨ï¼ˆéŸ³é¢‘ã€è§†è§‰ã€èåˆï¼‰çš„å‚æ•°ï¼Œå¹¶é€‚é…åˆ°å½“å‰æ¨¡å‹ä¸­ã€‚
        è¿™ä¸ªå‡½æ•°åœ¨æ¨¡å‹æ­å»ºå®Œæ¯•åç”±å¤–éƒ¨æµç¨‹ï¼ˆæ¯”å¦‚ BaseModel.setup()ï¼‰è°ƒç”¨ï¼›
        å®ƒå¹¶ä¸æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­æ¯è½®éƒ½ä¼šæ‰§è¡Œçš„ï¼Œè€Œæ˜¯ä¸€æ¬¡æ€§çš„åˆå§‹åŒ–è¾…åŠ©æ“ä½œã€‚
        """
        # called after model.setup()
        """
        åµŒå¥—å‡½æ•°ï¼šé”®åè½¬æ¢
        ä¸ºä»€ä¹ˆåŠ  'module.' å‰ç¼€ï¼Ÿ
        å½“ä½¿ç”¨ torch.nn.DataParallel æˆ– DistributedDataParallel å¹¶è¡Œè®­ç»ƒæ—¶ï¼Œæ¨¡å‹çš„å‚æ•°åå­—ä¼šå¤šä¸€ä¸ª 'module.' å‰ç¼€ï¼›
        æ‰€ä»¥åŠ è½½å•å¡ä¿å­˜çš„æ¨¡å‹åˆ°å¤šå¡æ¨¡å‹ï¼Œæˆ–ç›¸åæ—¶éœ€è¦æ‰‹åŠ¨å¤„ç†å‚æ•°åå¯¹é½ã€‚
        è¿™ä¸ªå‡½æ•°å°±æ˜¯æŠŠå‚æ•°ååŠ ä¸Š 'module.'ï¼Œé€‚é…è¿™ç§æƒ…å†µã€‚
        """
        def transform_key_for_parallel(state_dict):
            return OrderedDict([('module.' + key, value) for key, value in state_dict.items()])
        """
        æ¡ä»¶åˆ¤æ–­ï¼šä»…åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä½¿ç”¨
        åªæœ‰è®­ç»ƒæ¨¡å¼ä¸‹ï¼ˆå¦‚åœ¨ train.py ä¸­ï¼‰æ‰ä¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼›
        æ¨ç†/æµ‹è¯•é˜¶æ®µåˆ™ä¸éœ€è¦é‡å¤åˆå§‹åŒ–ã€‚
        """
        if self.isTrain:
            print('[ Init ] Load parameters from pretrained encoder network')
            """
            å…³é”®æ­¥éª¤ï¼šåŠ è½½å­ç½‘ç»œçš„é¢„è®­ç»ƒå‚æ•°
            è¿™äº›æ¨¡å—åˆ†åˆ«æ˜¯ï¼š
            netEmoAï¼šéŸ³é¢‘ LSTM ç¼–ç å™¨ï¼›
            netEmoVï¼šè§†é¢‘ LSTM ç¼–ç å™¨ï¼›
            netEmoFusionï¼šTransformer èåˆæ¨¡å—ã€‚
            è¿™é‡Œçš„ self.pretrained_encoder æ˜¯åœ¨å“ªå®šä¹‰çš„ï¼Ÿ
            ğŸ‘‰ å®ƒåº”å½“æ˜¯ä½ åœ¨ BaseModel æˆ–å…¶ä»–åˆå§‹åŒ–è¿‡ç¨‹ä¸­æŒ‡å®šçš„ä¸€ä¸ªå·²åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹å®ä¾‹ï¼ŒåŒ…å«è¿™äº›æ¨¡å—çš„å‚æ•°ã€‚
            """
            f = lambda x: transform_key_for_parallel(x)
            self.netEmoA.load_state_dict(f(self.pretrained_encoder.netEmoA.state_dict()))
            self.netEmoV.load_state_dict(f(self.pretrained_encoder.netEmoV.state_dict()))
            self.netEmoFusion.load_state_dict(f(self.pretrained_encoder.netEmoFusion.state_dict()))

    def load_from_opt_record(self, file_path):
        """
        è¯¥æ–¹æ³•ç”¨äºä» JSON æ–‡ä»¶ä¸­è¯»å–æ¨¡å‹é…ç½®ï¼ˆoptï¼‰è®°å½•ï¼Œå¹¶è¿˜åŸæˆ OptConfig å¯¹è±¡ã€‚
        ä½¿ç”¨åœºæ™¯ï¼š
        è¿™ä¸ªæ–¹æ³•å¯ä»¥è®©ä½ åœ¨è®­ç»ƒåé‡æ–°åŠ è½½ä¿å­˜çš„è¶…å‚æ•°é…ç½®ï¼Œæ¯”å¦‚ç”¨äºæ¨ç†ã€å¤ç°å®éªŒç­‰ï¼›
        å¸¸ä¸ä¿å­˜çš„æ¨¡å‹æƒé‡æ­é…ï¼Œç”¨äºæ„é€ æ¨¡å‹çš„å®Œæ•´çŠ¶æ€ï¼ˆç»“æ„ + å‚æ•° + é…ç½®ï¼‰ï¼›
        OptConfig åº”è¯¥æ˜¯ models.utils.config ä¸­å®šä¹‰çš„ç±»ï¼Œç”¨äºç»Ÿä¸€å­˜å‚¨å’Œç®¡ç†é…ç½®å‚æ•°ã€‚
        """
        opt_content = json.load(open(file_path, 'r'))# è½½å…¥ä»¥ JSON æ ¼å¼ä¿å­˜çš„è¶…å‚æ•°é…ç½®ï¼›æ–‡ä»¶å†…å®¹é€šå¸¸æ˜¯ dictï¼Œè®°å½•äº†æ¨¡å‹ç»“æ„ã€ç»´åº¦ã€å­¦ä¹ ç‡ç­‰ä¿¡æ¯ã€‚
        opt = OptConfig()
        opt.load(opt_content)
        return opt

    def set_input(self, input):
        """
        è¯¥æ–¹æ³•æ˜¯è®­ç»ƒ/æµ‹è¯•æ—¶æ¨¡å‹æ¥æ”¶ä¸€ä¸ª batch æ•°æ®çš„å…¥å£ï¼Œä¸»è¦æ˜¯å°†è¾“å…¥æ•°æ®æ”¾å…¥æ¨¡å‹çš„å˜é‡ä¸­ï¼Œå¹¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ï¼ˆGPU/CPUï¼‰ã€‚
        input æ˜¯ä¸€ä¸ª å­—å…¸ï¼Œæ¥è‡ª AudioVisualDataset çš„ __getitem__()ï¼›
        åŒ…å«é”®ï¼š
        'A_feat'ï¼šéŸ³é¢‘ç‰¹å¾å¼ é‡ (batch_size, T, D)
        'V_feat'ï¼šè§†é¢‘ç‰¹å¾å¼ é‡ (batch_size, T, D)
        'emo_label'ï¼šåˆ†ç±»æ ‡ç­¾ï¼ˆæ•´æ•°ï¼‰
        'personalized_feat'ï¼šä¸ªæ€§åŒ–åµŒå…¥ (batch_size, 1024)ï¼Œå¯é€‰
        """
        self.acoustic = input['A_feat'].float().to(self.device)# è®¾ç½®éŸ³é¢‘ç‰¹å¾
        self.visual = input['V_feat'].float().to(self.device)# è®¾ç½®è§†é¢‘ç‰¹å¾

        self.emo_label = input['emo_label'].to(self.device)# è®¾ç½®æƒ…æ„Ÿæ ‡ç­¾

        if 'personalized_feat' in input:# å¤„ç†ä¸ªæ€§åŒ–ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            self.personalized = input['personalized_feat'].float().to(self.device)
        else:
            self.personalized = None  # if no personalized features given

    def forward(self, acoustic_feat=None, visual_feat=None):
        if acoustic_feat is not None:
            self.acoustic = acoustic_feat.float().to(self.device)
            self.visual = visual_feat.float().to(self.device)

        # acoustic: [B, T, 9, D]
        B, T, W, D = self.acoustic.shape
        acoustic_list = []

        for i in range(T):
            frame_audio = self.acoustic[:, i, :, :]  # [B, 9, D]
            fused = self.audio_attention(frame_audio)  # [B, D]
            acoustic_list.append(fused)

        acoustic_aligned = torch.stack(acoustic_list, dim=1)  # [B, T, D]

        # ç¼–ç éŸ³é¢‘å’Œè§†é¢‘
        emo_feat_A = self.netEmoA(acoustic_aligned)  # [B, T, embd_size]
        emo_feat_V = self.netEmoV(self.visual)  # [B, T, embd_size]

        # èåˆ
        emo_fusion_feat = torch.cat((emo_feat_V, emo_feat_A), dim=-1)  # [B, T, 2*embd_size]
        emo_fusion_feat = self.netEmoFusion(emo_fusion_feat)  # [B, T, hidden]

        # reshape
        batch_size = emo_fusion_feat.size(0)
        emo_fusion_feat = emo_fusion_feat.permute(1, 0, 2).reshape(batch_size, -1)  # [B, T*hidden]

        # æ‹¼æ¥ä¸ªæ€§åŒ–ç‰¹å¾
        if self.personalized is not None:
            emo_fusion_feat = torch.cat((emo_fusion_feat, self.personalized), dim=-1)  # [B, T*hidden + 1024]

        # åˆ†ç±»å™¨è¾“å‡º
        self.emo_logits_fusion, _ = self.netEmoCF(emo_fusion_feat)
        self.emo_logits, _ = self.netEmoC(emo_fusion_feat)
        self.emo_pred = F.softmax(self.emo_logits, dim=-1)

    def backward(self):
        """
        è¿™æ®µ backward() æ–¹æ³•å®šä¹‰çš„æ˜¯æ¨¡å‹çš„åå‘ä¼ æ’­é˜¶æ®µï¼Œå®ƒå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        âœ… è®¡ç®—æŸå¤± â†’ âœ… åˆæˆæ€»æŸå¤± â†’ âœ… åå‘ä¼ æ’­ â†’ âœ… æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        åœ¨ PyTorch ä¸­ï¼Œbackward() æ˜¯è®­ç»ƒå¾ªç¯ä¸­çš„æ ¸å¿ƒæ­¥éª¤ä¹‹ä¸€ï¼›
        é€šå¸¸ä¼šåœ¨è°ƒç”¨ forward() åæ‰§è¡Œï¼Œä»¥ä¾¿åå‘ä¼ æ’­è¯¯å·®ã€æ›´æ–°æ¨¡å‹æƒé‡ã€‚
        """
        """Calculate the loss for back propagation"""
        self.loss_emo_CE = self.criterion_ce(self.emo_logits, self.emo_label)
        """
        è®¡ç®—ä¸»æŸå¤±ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆä¸»åˆ†ç±»å™¨ï¼‰
        self.emo_logits æ˜¯ä¸»åˆ†ç±»å™¨ netEmoC çš„è¾“å‡ºï¼›
        self.emo_label æ˜¯ ground truth æ ‡ç­¾ï¼›
        ä½¿ç”¨ CrossEntropyLoss() è®¡ç®—åˆ†ç±»æŸå¤±ï¼›
        è¿™æ˜¯æ ‡å‡†çš„å¤šåˆ†ç±» lossã€‚
        """
        self.loss_EmoF_CE = self.focal_weight * self.criterion_focal(self.emo_logits_fusion, self.emo_label)
        """
        è®¡ç®—è¾…åŠ©æŸå¤±ï¼šFocal Lossï¼ˆå¢å¼ºæ¨¡å‹å¯¹å°ç±»/å›°éš¾æ ·æœ¬çš„å…³æ³¨ï¼‰
        self.emo_logits_fusion æ˜¯è¾…åŠ©åˆ†ç±»å™¨ netEmoCF çš„è¾“å‡ºï¼›
        self.criterion_focal æ˜¯ focal loss å®ä¾‹ï¼ˆç”¨äºç±»åˆ«ä¸å¹³è¡¡åœºæ™¯ï¼‰ï¼›
        self.focal_weight æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨æ¥è°ƒèŠ‚ focal loss åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ï¼›
        è¿™éƒ¨åˆ†å¢å¼ºæ¨¡å‹å¯¹éš¾ä»¥é¢„æµ‹æˆ–å°æ ·æœ¬ç±»åˆ«çš„å­¦ä¹ èƒ½åŠ›ã€‚
        """
        loss = self.loss_emo_CE + self.loss_EmoF_CE
        loss.backward()
        """
        åˆæˆæ€»æŸå¤±å¹¶åå‘ä¼ æ’­
        æ€»æŸå¤±æ˜¯ä¸»åˆ†ç±»å™¨æŸå¤± + è¾…åŠ©åˆ†ç±»å™¨æŸå¤±ï¼›
        è°ƒç”¨ .backward() è‡ªåŠ¨è®¡ç®—æ¯ä¸ªæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼ˆautogradï¼‰ã€‚
        """
        for model in self.model_names:
            torch.nn.utils.clip_grad_norm_(getattr(self, 'net' + model).parameters(), 1.0)
        """
        æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        éå†æ¨¡å‹åç§°åˆ—è¡¨ï¼ˆ['EmoA', 'EmoV', 'EmoFusion', 'EmoC', 'EmoCF']ï¼‰ï¼›
        getattr(self, 'net' + model) åŠ¨æ€è·å–å¯¹åº”æ¨¡å‹å­æ¨¡å—ï¼›
        ä½¿ç”¨ clip_grad_norm_() é™åˆ¶æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ L2 èŒƒæ•°ä¸è¶…è¿‡ 1.0ï¼›
        è¿™æ˜¯é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„ä¸€ç§å¸¸è§æŠ€æœ¯ï¼›
        ç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨ LSTMã€Transformer æ—¶å®¹æ˜“é‡åˆ°æ­¤é—®é¢˜ã€‚
        """

    def optimize_parameters(self, epoch):
        """
        è¿™æ®µ optimize_parameters() æ˜¯ä½ æ¨¡å‹è®­ç»ƒæµç¨‹ä¸­çš„æ ¸å¿ƒæ‰§è¡Œå‡½æ•°ï¼Œæ¯æ¬¡è®­ç»ƒè¿­ä»£ï¼ˆå³æ¯ä¸ª batchï¼‰éƒ½ä¼šè°ƒç”¨å®ƒã€‚
        å®ƒè´Ÿè´£ï¼šå‰å‘ä¼ æ’­ â†’ æ¸…é›¶æ¢¯åº¦ â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°æƒé‡
        å³å®Œæˆä¸€æ•´ä¸ª â€œå‰ä¼  + åä¼  + ä¼˜åŒ–â€ è¿‡ç¨‹ã€‚
        å‚æ•° epochï¼šå½“å‰è®­ç»ƒçš„ epoch ç¼–å·ï¼ˆè™½ç„¶åœ¨æœ¬å‡½æ•°å†…æ²¡æœ‰ç›´æ¥ç”¨ï¼Œä½†å¯èƒ½ç”¨äºå†…éƒ¨è®°å½•ã€è°ƒè¯•æˆ–æ—¥å¿—æ‰“å°ï¼‰ï¼›
        è¿™ä¸ªå‡½æ•°é€šå¸¸ç”±å¤–éƒ¨è®­ç»ƒå¾ªç¯ for epoch in range(...) æ¯æ¬¡è°ƒç”¨ã€‚
        """
        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        # forwardå‰å‘ä¼ æ’­
        self.forward()
        """
        è°ƒç”¨æ¨¡å‹çš„ forward() æ–¹æ³•ï¼š
        ç¼–ç éŸ³é¢‘+è§†é¢‘ç‰¹å¾ï¼›
        Transformer èåˆï¼›
        æ‹¼æ¥ä¸ªæ€§åŒ–å‘é‡ï¼›
        åˆ†ç±»å™¨è¾“å‡º logitsï¼›
        è®¡ç®— softmax å¾—åˆ°é¢„æµ‹æ¦‚ç‡ï¼ˆself.emo_predï¼‰ï¼›
        forward() å†…éƒ¨å·²ç»å°† self.emo_logits, self.emo_logits_fusion ç­‰å‡†å¤‡å¥½ä¾›åç»­ä½¿ç”¨ã€‚
        """
        # backward
        self.optimizer.zero_grad()# æ¢¯åº¦æ¸…é›¶,æ¸…ç©ºä¸Šä¸€æ­¥æ®‹ç•™çš„æ¢¯åº¦ï¼›
        self.backward()
        """
        åå‘ä¼ æ’­
        è°ƒç”¨ self.backward()ï¼š
        è®¡ç®—ä¸»æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰å’Œè¾…åŠ©æŸå¤±ï¼ˆFocal Lossï¼‰ï¼›
        åˆå¹¶ä¸¤ä¸ªæŸå¤±ï¼›
        .backward() æ‰§è¡Œ autograd è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼›
        ä½¿ç”¨ clip_grad_norm_() å¯¹æ‰€æœ‰æ¨¡å—æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚
        """
        self.optimizer.step()# æ›´æ–°å‚æ•°,ä½¿ç”¨ä¹‹å‰æ³¨å†Œçš„ Adam ä¼˜åŒ–å™¨ï¼šå°†åˆšåˆšè®¡ç®—å¥½çš„æ¢¯åº¦ç”¨äºæ›´æ–°æ¨¡å‹ä¸­çš„å‚æ•°ã€‚


class ActivateFun(torch.nn.Module):
    """
    ä½ è¿™æ®µä»£ç å®šä¹‰äº†ä¸¤ä¸ªç±»ï¼Œåˆ†åˆ«å°è£…äº†ï¼š
    âœ… è‡ªå®šä¹‰æ¿€æ´»å‡½æ•°æ¨¡å— ActivateFun
    âœ… æ”¹è‰¯ç‰ˆåˆ†ç±»æŸå¤±å‡½æ•° Focal_Lossï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼‰
    """
    def __init__(self, opt):
        super(ActivateFun, self).__init__()
        self.activate_fun = opt.activate_fun
    """
    ActivateFunï¼šæ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©æ¿€æ´»å‡½æ•°
    æ¥æ”¶ä¸€ä¸ªé…ç½®å¯¹è±¡ optï¼›
    opt.activate_fun æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "relu"ã€"gelu"ï¼‰ï¼ŒæŒ‡ç¤ºä½¿ç”¨å“ªç§æ¿€æ´»å‡½æ•°ï¼›
    å°†æ¿€æ´»å‡½æ•°ç±»å‹è®°å½•åœ¨ self.activate_fun ä¸­ã€‚
    """
    def _gelu(self, x):
        return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))
    """
    å†…éƒ¨å®šä¹‰çš„ GELU å‡½æ•°ï¼ˆä¸æ˜¯é»˜è®¤ APIï¼‰
    è¿™æ˜¯ GELUï¼ˆGaussian Error Linear Unitï¼‰çš„ç²¾ç¡®å…¬å¼ç‰ˆæœ¬ï¼›
    æ¯” ReLU æ›´å¹³æ»‘ï¼Œåœ¨æŸäº›ä»»åŠ¡ï¼ˆå¦‚ NLPã€BERTï¼‰ä¸­è¡¨ç°æ›´ä¼˜ã€‚
    """
    def forward(self, x):
        if self.activate_fun == 'relu':
            return torch.relu(x)
        elif self.activate_fun == 'gelu':
            return self._gelu(x)
    """
    forward()ï¼šæ ¹æ®é…ç½®é€‰æ‹©æ¿€æ´»å‡½æ•°æ‰§è¡Œ
    æ ¹æ®é…ç½®è¿”å› ReLU æˆ– GELU æ¿€æ´»ï¼›
    å¦‚æœä»¥åä½ æƒ³æ‰©å±•æ›´å¤šæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ LeakyReLUã€Swishï¼‰ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿åŠ è¿›æ¥ã€‚
    """

class Focal_Loss(torch.nn.Module):
    def __init__(self, weight=0.5, gamma=3, reduction='mean'):
        super(Focal_Loss, self).__init__()
        self.gamma = gamma
        self.alpha = weight
        self.reduction = reduction
    """
    Focal_Lossï¼šå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æ”¹è‰¯æŸå¤±å‡½æ•°
    å‚æ•°	            ä½œç”¨
    weightï¼ˆæˆ– Î±ï¼‰	ç±»åˆ«æƒé‡ï¼ˆå¯¹æ­£ç±»æˆ–å›°éš¾æ ·æœ¬çš„å¼ºè°ƒï¼‰
    gamma	        è°ƒèŠ‚å…³æ³¨éš¾æ˜“æ ·æœ¬çš„ç¨‹åº¦
    reduction	    è¾“å‡ºæ–¹å¼ï¼ˆ'mean' / 'sum' / 'none'ï¼‰
    Focal Loss æ˜¯ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­å¯¹å®¹æ˜“åˆ†ç±»çš„æ ·æœ¬é™ä½æƒé‡ï¼Œå¯¹å›°éš¾æ ·æœ¬æé«˜æƒé‡ã€‚
    """
    def forward(self, preds, targets):
        """
        preds:softmax output
        labels:true values
        """
        ce_loss = F.cross_entropy(preds, targets, reduction='mean')# è®¡ç®—æ™®é€šäº¤å‰ç†µæŸå¤±
        pt = torch.exp(-ce_loss)# è®¡ç®— pt = softmax æ¦‚ç‡
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss# æ„é€  Focal Loss

        if self.reduction == 'none':
            return focal_loss
        elif self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            raise NotImplementedError("Invalid reduction mode. Please choose 'none', 'mean', or 'sum'.")
        """
        noneï¼šè¿”å›æ¯ä¸ªæ ·æœ¬çš„ lossï¼›
        meanï¼ˆé»˜è®¤ï¼‰ï¼šæ±‚å¹³å‡ï¼›
        sumï¼šæ±‚å’Œï¼›
        """